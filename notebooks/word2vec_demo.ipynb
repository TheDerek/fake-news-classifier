{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup & Intro\n",
    "Created with guidance from: https://www.tensorflow.org/tutorials/word2vec\n",
    "\n",
    "- Import the nessary libraries\n",
    "- Create the constants\n",
    "- Setup the tensorflow interactive session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "VOCABULARY_SIZE = 5000\n",
    "EMBEDDING_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_SAMPLED = 64\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "def print_tensors(tensors: {str: tf.Variable}):\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for name, tensor in tensors.items():\n",
    "        print('{}:'.format(name))\n",
    "        print(tensor)\n",
    "        print(sess.run(tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the graph\n",
    "- Define the `embeddings` matrix as a big random matrix to start, initalised as a uniform unit cube\n",
    "- Define the `weights` between each word in the vocabulary and the embeddings\n",
    "- Define the `biases` for each word in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings:\n",
      "<tf.Variable 'Variable_33:0' shape=(100, 100) dtype=float32_ref>\n",
      "[[-0.99959564  0.17514896  0.6023774  ... -0.7602339   0.20073867\n",
      "  -0.20027566]\n",
      " [ 0.36964917 -0.20007443  0.08079648 ... -0.94977164 -0.8610573\n",
      "   0.01886177]\n",
      " [-0.13537192 -0.02241683 -0.22921944 ...  0.02772141 -0.20781112\n",
      "  -0.72739315]\n",
      " ...\n",
      " [ 0.76542735 -0.48729515  0.06938696 ... -0.43892407  0.47028303\n",
      "   0.6520159 ]\n",
      " [ 0.32232738  0.5187137  -0.15069056 ...  0.21919918  0.78081894\n",
      "  -0.6112175 ]\n",
      " [ 0.13921404  0.1475718  -0.07931185 ...  0.06942058 -0.08953094\n",
      "  -0.8860445 ]]\n",
      "Weights:\n",
      "<tf.Variable 'Variable_34:0' shape=(100, 100) dtype=float32_ref>\n",
      "[[-0.11367207  0.08951253  0.03935289 ... -0.05828243  0.07683927\n",
      "   0.00589061]\n",
      " [-0.13877697 -0.03214059 -0.11897998 ... -0.04240558 -0.03036506\n",
      "   0.17767867]\n",
      " [-0.16628455 -0.07077862  0.01699749 ...  0.09565924 -0.12992476\n",
      "  -0.05562282]\n",
      " ...\n",
      " [-0.08442713  0.05986143  0.01399434 ... -0.02756865  0.1139672\n",
      "  -0.0425541 ]\n",
      " [ 0.1206388  -0.10573912  0.06333264 ... -0.08541632 -0.02753634\n",
      "   0.03725084]\n",
      " [-0.14484213 -0.16365077  0.05473015 ... -0.01520055  0.03298864\n",
      "  -0.04937994]]\n",
      "Biases:\n",
      "<tf.Variable 'Variable_35:0' shape=(100,) dtype=float32_ref>\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([VOCABULARY_SIZE, EMBEDDING_SIZE], -1.0, 1.0))\n",
    "weights = tf.Variable(tf.truncated_normal([VOCABULARY_SIZE, EMBEDDING_SIZE],\n",
    "                                          stddev=1.0 / math.sqrt(EMBEDDING_SIZE)))\n",
    "biases = tf.Variable(tf.zeros([VOCABULARY_SIZE]))\n",
    "\n",
    "print_tensors({'Embeddings': embeddings, 'Weights': weights, 'Biases': biases})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the inputs\n",
    "- Each word is represented as an integer, so the the data for `train_inputs` will look like:\n",
    "  `[[41, 12, 42...], [...], ...]`\n",
    "- `train_labels` is a list of labels for each document in `train_inputs`. In this case it is the next word in the    sequence in the format of `[[41], [4], ...]`\n",
    "- `embed` is a lookup matrix of the document vocabulary combined with the emeddings. Each row of the matrix are the embeddings for that word in the `train_input` document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed:\n",
      "Tensor(\"embedding_lookup_2:0\", shape=(3, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "print('Embed:\\n{}'.format(embed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the loss function used for training\n",
    "\n",
    "This is too define how accurate the weights and biases are at predicting the `train_labels` given the `embed` inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(\n",
    "  tf.nn.nce_loss(weights=weights,\n",
    "                 biases=biases,\n",
    "                 labels=train_labels,\n",
    "                 inputs=embed,\n",
    "                 num_sampled=NUM_SAMPLED,\n",
    "                 num_classes=VOCABULARY_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dataset here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate batches of data to use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        context_words = [w for w in range(span) if w != skip_window]\n",
    "        words_to_use = random.sample(context_words, num_skips)\n",
    "        for j, context_word in enumerate(words_to_use):\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[context_word]\n",
    "        if data_index == len(data):\n",
    "            buffer[:] = data[:span]\n",
    "            data_index = span\n",
    "        else:\n",
    "            buffer.append(data[data_index])\n",
    "            data_index += 1\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-32e21a770bf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_skips\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-9dbcdb566f61>\u001b[0m in \u001b[0;36mgenerate_batch\u001b[0;34m(batch_size, num_skips, skip_window)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mspan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mskip_window\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# [ skip_window target skip_window ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mdata_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mspan\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mdata_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdata_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "for inputs, labels in generate_batch(batch_size=8, num_skips=2, skip_window=1):\n",
    "  feed_dict = {train_inputs: inputs, train_labels: labels}\n",
    "  _, cur_loss = session.run([optimizer, loss], feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
